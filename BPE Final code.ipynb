{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.word_vocab = {}\n",
    "        self.token_vocab = set()\n",
    "\n",
    "    def get_vocab(self, text):\n",
    "        return set(char for word in text.translate(str.maketrans('', '', punctuation)).split() for char in word)\n",
    "\n",
    "    def count_words(self, text):\n",
    "        freqs = defaultdict(int)\n",
    "        clean_text = text.translate(str.maketrans('', '', punctuation))\n",
    "        \n",
    "        for word in clean_text.split():\n",
    "            word += \"_\"\n",
    "            freqs[word] += 1\n",
    "        \n",
    "        return freqs\n",
    "\n",
    "    def find_pairs(self, word_dict):\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word, count in word_dict.items():\n",
    "            tokens = word.split()\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pairs[pair] += count\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, word_dict, pair):\n",
    "        new_dict = defaultdict(int)\n",
    "        old = \" \".join(pair)\n",
    "        new = \"\".join(pair)\n",
    "        \n",
    "        for word, count in word_dict.items():\n",
    "            merged_word = word.replace(old, new)\n",
    "            new_dict[merged_word] += count\n",
    "            \n",
    "        return new_dict\n",
    "\n",
    "    def train(self, text, num_merges):\n",
    "        self.token_vocab = self.get_vocab(text)\n",
    "        \n",
    "        word_freqs = self.count_words(text)\n",
    "        self.word_vocab = {' '.join(word): freq for word, freq in word_freqs.items()}\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pair_freqs = self.find_pairs(self.word_vocab)\n",
    "            \n",
    "            if not pair_freqs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pair_freqs.items(), key=lambda x: x[1])[0]\n",
    "            merged_token = ''.join(best_pair)\n",
    "            \n",
    "            self.token_vocab.add(merged_token)\n",
    "            self.word_vocab = self.merge_pair(self.word_vocab, best_pair)\n",
    "        \n",
    "        return self.word_vocab, self.token_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Books:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading gutenberg: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/dhruvgorasiya/nltk_data/corpora/gutenberg/hakespeare-hamlet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[914], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m book1 \u001b[38;5;241m=\u001b[39m gutenberg\u001b[38;5;241m.\u001b[39mraw(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mausten-emma.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m book2 \u001b[38;5;241m=\u001b[39m gutenberg\u001b[38;5;241m.\u001b[39mraw(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblake-poems.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m book3 \u001b[38;5;241m=\u001b[39m \u001b[43mgutenberg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhakespeare-hamlet.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(book1)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"\\nFinal word vocabulary: {final_word_vocab}\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"Complete token vocabulary: {complete_vocab}\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(f\"Vocabulary size: {len(complete_vocab)}\")\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[0;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:333\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[1;32m    332\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    309\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/Users/dhruvgorasiya/nltk_data/corpora/gutenberg/hakespeare-hamlet.txt'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nltk.download('gutenberg')\n",
    "    from nltk.corpus import gutenberg\n",
    "    book1 = gutenberg.raw(\"austen-emma.txt\")\n",
    "    book2 = gutenberg.raw(\"blake-poems.txt\")\n",
    "    book3 = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
    "    \n",
    "    print(book1)\n",
    "    \n",
    "    # print(f\"\\nFinal word vocabulary: {final_word_vocab}\")\n",
    "    # print(f\"Complete token vocabulary: {complete_vocab}\")\n",
    "    # print(f\"Vocabulary size: {len(complete_vocab)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
