{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/dhruvgorasiya/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhruvgorasiya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "from string import punctuation\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, text, num_merges):\n",
    "        self.token_vocab = set()\n",
    "        self.corpus = {}\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = set()\n",
    "        self.text = self.preprocess_text(text)\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # clean_text = ''.join(char for char in text if char not in punctuation)\n",
    "        clean_text = text\n",
    "        clean_text = ' '.join(clean_text.split())\n",
    "        return clean_text\n",
    "\n",
    "    def get_vocab(self):\n",
    "        vocab = set(char for word in self.text.split() for char in word)\n",
    "        vocab.add(\"_\")\n",
    "        return vocab\n",
    "\n",
    "    def count_words(self):\n",
    "        freqs = defaultdict(int)\n",
    "        for word in self.text.split():\n",
    "            word += \"_\"\n",
    "            freqs[word] += 1\n",
    "        return freqs\n",
    "\n",
    "    def find_pairs(self):\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word, count in self.corpus.items():\n",
    "            tokens = word.split()\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                pairs[pair] += count\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, pair):\n",
    "        new_dict = defaultdict(int)\n",
    "        bigram = \" \".join(pair)\n",
    "        merged = \"\".join(pair)\n",
    "        \n",
    "        for word, count in self.corpus.items():\n",
    "            new_word = word.replace(bigram, merged)\n",
    "            new_dict[new_word] = count\n",
    "            \n",
    "        return new_dict\n",
    "    \n",
    "    \n",
    "    def BPE(self):\n",
    "        self.token_vocab = self.get_vocab()\n",
    "        \n",
    "        word_freqs = self.count_words()\n",
    "        self.corpus = {' '.join(word): freq for word, freq in word_freqs.items()}\n",
    "        \n",
    "        for _ in range(self.num_merges):\n",
    "            pair_freqs = self.find_pairs()\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pair_freqs.items(), key=lambda x: x[1])[0]\n",
    "            self.merges.add(best_pair)\n",
    "            self.token_vocab.add(''.join(best_pair))\n",
    "            self.corpus = self.merge_pair(best_pair)\n",
    "            \n",
    "        return self.corpus, self.token_vocab, self.merges\n",
    "    \n",
    "    def encode(self, text):\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        result = []\n",
    "        \n",
    "        for word in processed_text.split():\n",
    "            chars = []\n",
    "            current_token = \"\"\n",
    "            \n",
    "            for char in word:\n",
    "                if char.isalnum() or char == '_':\n",
    "                    current_token += char\n",
    "                else:\n",
    "                    if current_token:\n",
    "                        chars.append(current_token)\n",
    "                        current_token = \"\"\n",
    "                    chars.append(char)\n",
    "            \n",
    "            if current_token != \"\":\n",
    "                chars.append(current_token)\n",
    "            \n",
    "            for token in chars:\n",
    "                token_with_suffix = f\"{token}_\"\n",
    "                \n",
    "                if token_with_suffix in self.corpus:\n",
    "                    result.append(token_with_suffix)\n",
    "                    continue\n",
    "                    \n",
    "                current = ' '.join(token_with_suffix)\n",
    "                \n",
    "                while True:\n",
    "                    performed_merge = False\n",
    "                    \n",
    "                    for merge_pair in self.merges:\n",
    "                        merge_str = ' '.join(merge_pair)\n",
    "                        if merge_str in current:\n",
    "                            merged = ''.join(merge_pair)\n",
    "                            current = current.replace(merge_str, merged)\n",
    "                            performed_merge = True\n",
    "                            \n",
    "                    if not performed_merge:\n",
    "                        break\n",
    "                        \n",
    "                result.extend(current.split())\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def decode(self, encoded_text):\n",
    "        return ' '.join(self.token_vocab[i] for i in encoded_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    book1 = gutenberg.raw(\"austen-emma.txt\")\n",
    "    book2 = gutenberg.raw(\"blake-poems.txt\") \n",
    "    book3 = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
    "    \n",
    "    training_text = book1 + \" \" + book2 + \" \" + book3\n",
    "    \n",
    "    test_book1 = gutenberg.raw(\"shakespeare-caesar.txt\")\n",
    "    test_book2 = gutenberg.raw(\"carroll-alice.txt\")\n",
    "    test_book3 = gutenberg.raw(\"chesterton-ball.txt\")\n",
    "    \n",
    "    reference_tokenizations = {\n",
    "        'shakespeare-caesar': word_tokenize(test_book1),\n",
    "        'carroll-alice': word_tokenize(test_book2),\n",
    "        'chesterton-ball': word_tokenize(test_book3)\n",
    "    }\n",
    "        \n",
    "    num_merges = 100000\n",
    "    bpe_tokenizer = BPETokenizer(training_text, num_merges)\n",
    "    corpus, vocab, merges = bpe_tokenizer.BPE()\n",
    "    \n",
    "    def calculate_metrics(reference_tokens, bpe_tokens):\n",
    "        ref_vocab = set(reference_tokens)\n",
    "        bpe_vocab = set(bpe_tokens)\n",
    "        \n",
    "        true_positives = len(ref_vocab.intersection(bpe_vocab))\n",
    "        false_positives = len(bpe_vocab - ref_vocab) \n",
    "        false_negatives = len(ref_vocab - bpe_vocab)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        jaccard_similarity = len(ref_vocab.intersection(bpe_vocab)) / len(ref_vocab.union(bpe_vocab)) if ref_vocab or bpe_vocab else 0\n",
    "\n",
    "        tokenization_accuracy = ((true_positives / len(bpe_tokens)) * 100) if len(bpe_tokens) > 0 else 0\n",
    "        tokenization_coverage = ((len(bpe_vocab) / len(ref_vocab)) * 100) if len(ref_vocab) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'ref_vocab_size': len(ref_vocab),\n",
    "            'bpe_vocab_size': len(bpe_vocab),\n",
    "            'ref_avg_token_length': sum(len(t) for t in reference_tokens) / len(reference_tokens),\n",
    "            'bpe_avg_token_length': sum(len(t) for t in bpe_tokens) / len(bpe_tokens),\n",
    "            'total_ref_tokens': len(reference_tokens),\n",
    "            'total_bpe_tokens': len(bpe_tokens),\n",
    "            'tokenization_accuracy': tokenization_accuracy,\n",
    "            'tokenization_coverage': tokenization_coverage,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'jaccard_similarity': jaccard_similarity,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    print(\"\\nTokenization Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "for book_name, ref_tokens in reference_tokenizations.items():\n",
    "        test_book = gutenberg.raw(f\"{book_name}.txt\")\n",
    "        bpe_tokens = bpe_tokenizer.encode(test_book)\n",
    "        \n",
    "        bpe_tokens = [token.replace('_', '') for token in bpe_tokens]\n",
    "        \n",
    "        print(bpe_tokens)\n",
    "        print(ref_tokens)\n",
    "                \n",
    "        metrics = calculate_metrics(ref_tokens, bpe_tokens)\n",
    "        \n",
    "        print(f\"\\nResults for {book_name}:\")\n",
    "        print(f\"Reference vocabulary size: {metrics['ref_vocab_size']}\")\n",
    "        print(f\"BPE vocabulary size: {metrics['bpe_vocab_size']}\")\n",
    "        print(f\"Reference avg token length: {metrics['ref_avg_token_length']:.2f}\")\n",
    "        print(f\"BPE avg token length: {metrics['bpe_avg_token_length']:.2f}\")\n",
    "        print(f\"Total reference tokens: {metrics['total_ref_tokens']}\")\n",
    "        print(f\"Total BPE tokens: {metrics['total_bpe_tokens']}\")\n",
    "        print(f\"Tokenization accuracy: {metrics['tokenization_accuracy']:.2f}%\")\n",
    "        print(f\"Tokenization coverage: {metrics['tokenization_coverage']:.2f}%\")\n",
    "        print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"F1 score: {metrics['f1_score']:.2f}\")\n",
    "        print(f\"Jaccard similarity: {metrics['jaccard_similarity']:.2f}\")\n",
    "        print(f\"True positives: {metrics['true_positives']}\")\n",
    "        print(f\"False positives: {metrics['false_positives']}\")\n",
    "        print(f\"False negatives: {metrics['false_negatives']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
