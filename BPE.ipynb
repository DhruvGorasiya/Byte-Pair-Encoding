{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/dhruvgorasiya/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhruvgorasiya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, text, num_merges):\n",
    "        self.token_vocab = set()\n",
    "        self.corpus = {}\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = set()\n",
    "        self.text = self._preprocess_text(text)\n",
    "        \n",
    "    def _preprocess_text(self, text):\n",
    "        words = []\n",
    "        current_word = ''\n",
    "        \n",
    "        for char in text:\n",
    "            if char.isspace() or char in punctuation:\n",
    "                if current_word:\n",
    "                    words.append(current_word)\n",
    "                    current_word = ''\n",
    "                if not char.isspace():\n",
    "                    words.append(char)\n",
    "            else:\n",
    "                current_word += char\n",
    "                \n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        vocab = set(char for word in self.text.split() for char in word)\n",
    "        vocab.add(\"_\")\n",
    "        return vocab\n",
    "\n",
    "    def count_words(self):\n",
    "        freqs = defaultdict(int)\n",
    "        for word in self.text.split():\n",
    "            word += \"_\"\n",
    "            freqs[word] += 1\n",
    "        return freqs\n",
    "    \n",
    "    def find_pairs(self):\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word, count in self.corpus.items():\n",
    "            tokens = word.split()\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                overlap = sum(1 for c in tokens[i] if c in tokens[i + 1])\n",
    "                merged_len = len(tokens[i]) + len(tokens[i + 1]) - overlap\n",
    "                pair_score = count * (1 + overlap/merged_len)\n",
    "                pairs[pair] += pair_score\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, pair):\n",
    "        new_dict = defaultdict(int)\n",
    "        bigram = \" \".join(pair)\n",
    "        merged = \"\".join(pair)\n",
    "        \n",
    "        for word, count in self.corpus.items():\n",
    "            new_word = word.replace(bigram, merged)\n",
    "            new_dict[new_word] = count\n",
    "            \n",
    "        return new_dict\n",
    "\n",
    "    def BPE(self):\n",
    "        vocab = self.get_vocab()\n",
    "        self.corpus = {\" \".join(list(word)): count \n",
    "                      for word, count in self.count_words().items()}\n",
    "\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.find_pairs()\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            self.corpus = self.merge_pair(best_pair)\n",
    "            self.merges.add(best_pair)\n",
    "            \n",
    "        return self.corpus, vocab, self.merges\n",
    "\n",
    "    def encode(self, text):\n",
    "        processed_text = self._preprocess_text(text)\n",
    "        result = []\n",
    "        merge_dict = {\" \".join(pair): \"\".join(pair) for pair in self.merges}\n",
    "        \n",
    "        for word in processed_text.split():\n",
    "            word += \"_\"\n",
    "            current = \" \".join(list(word))\n",
    "            \n",
    "            while True:\n",
    "                merged = False\n",
    "                parts = current.split()\n",
    "                \n",
    "                for i in range(len(parts) - 1):\n",
    "                    bigram = f\"{parts[i]} {parts[i+1]}\"\n",
    "                    if bigram in merge_dict:\n",
    "                        current = current.replace(bigram, merge_dict[bigram], 1)\n",
    "                        merged = True\n",
    "                        break\n",
    "                        \n",
    "                if not merged:\n",
    "                    break\n",
    "                    \n",
    "            result.append(current.replace(\" \", \"\"))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        decoded_text = \"\".join(\"\".join(token) for token in encoded_text)\n",
    "        return decoded_text.replace(\"_\", \" \")\n",
    "\n",
    "    def calculate_metrics(self, reference_tokens, bpe_tokens):\n",
    "        ref_vocab = set(reference_tokens)\n",
    "        bpe_vocab = set(bpe_tokens)\n",
    "        \n",
    "        true_positives = len(ref_vocab.intersection(bpe_vocab))\n",
    "        false_positives = len(bpe_vocab - ref_vocab) \n",
    "        false_negatives = len(ref_vocab - bpe_vocab)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        jaccard_similarity = len(ref_vocab.intersection(bpe_vocab)) / len(ref_vocab.union(bpe_vocab)) if ref_vocab or bpe_vocab else 0\n",
    "        \n",
    "        correct_tokens = sum(1 for token in bpe_vocab if token in ref_vocab)\n",
    "        tokenization_accuracy = (correct_tokens / len(ref_vocab.union(bpe_vocab))) * 100 if len(ref_vocab.union(bpe_vocab)) > 0 else 0\n",
    "        tokenization_coverage = (len(bpe_vocab) / len(ref_vocab)) * 100 if len(ref_vocab) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'correct_tokens': correct_tokens,\n",
    "            'ref_vocab_size': len(ref_vocab),\n",
    "            'bpe_vocab_size': len(bpe_vocab),\n",
    "            'ref_avg_token_length': sum(len(t) for t in reference_tokens) / len(reference_tokens),\n",
    "            'bpe_avg_token_length': sum(len(t) for t in bpe_tokens) / len(bpe_tokens),\n",
    "            'total_ref_tokens': len(reference_tokens),\n",
    "            'total_bpe_tokens': len(bpe_tokens),\n",
    "            'tokenization_accuracy': tokenization_accuracy,\n",
    "            'tokenization_coverage': tokenization_coverage,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'jaccard_similarity': jaccard_similarity,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low_', 'asdf_']\n",
      "Low asdf \n"
     ]
    }
   ],
   "source": [
    "training_text = \"Low lower lowest new newer\"\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(training_text, 10)\n",
    "\n",
    "bpe_tokenizer.BPE()\n",
    "\n",
    "test_text = \"Low asdf\"\n",
    "\n",
    "print(bpe_tokenizer.encode(test_text))\n",
    "print(bpe_tokenizer.decode(bpe_tokenizer.encode(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    books = {\n",
    "    'training': [\n",
    "        gutenberg.raw(\"austen-emma.txt\"),\n",
    "        gutenberg.raw(\"blake-poems.txt\"),\n",
    "        gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
    "    ],\n",
    "    'testing': {\n",
    "        'shakespeare-caesar': gutenberg.raw(\"shakespeare-caesar.txt\"),\n",
    "        'carroll-alice': gutenberg.raw(\"carroll-alice.txt\"),\n",
    "        'chesterton-ball': gutenberg.raw(\"chesterton-ball.txt\")\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    training_text = \" \".join(books['training'])\n",
    "    bpe_tokenizer = BPETokenizer(training_text, num_merges=100000)\n",
    "    corpus, vocab, merges = bpe_tokenizer.BPE()\n",
    "    \n",
    "    print(\"\\nTokenization Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for book_name, test_book in books['testing'].items():\n",
    "        ref_tokens = word_tokenize(test_book)\n",
    "        bpe_tokens = bpe_tokenizer.encode(test_book)\n",
    "        bpe_tokens = [token.replace('_', '') for token in bpe_tokens]\n",
    "        \n",
    "        metrics = bpe_tokenizer.calculate_metrics(ref_tokens, bpe_tokens)\n",
    "        \n",
    "        print(f\"\\nResults for {book_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"{metric.replace('_', ' ').title()}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
